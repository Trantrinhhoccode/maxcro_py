import google.generativeai as genai
import requests
import time
import os
import re
import hashlib
import unicodedata
from datetime import datetime, date, timedelta
from urllib.parse import quote_plus
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    print("Warning: beautifulsoup4 not installed. Full article crawling will be disabled.")

# ================= C·∫§U H√åNH (ƒêI·ªÄN L·∫†I TH√îNG TIN C·ª¶A B·∫†N) =================
# Kh√¥ng hardcode key/token trong file. Set b·∫±ng bi·∫øn m√¥i tr∆∞·ªùng:
# export GEMINI_API_KEY="..."
# export TELEGRAM_TOKEN="..."
# export TELEGRAM_CHAT_ID="6628382207"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip()
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").strip()  # d·∫°ng s·ªë, string ok

# Google News RSS sources (d√πng search + site:domain)
GOOGLE_SOURCES = [
    "vietstock.vn",
    "vneconomy.vn",
    "cafef.vn",
    "ndh.vn",
]

# ================= THEO D√ïI C·ªî PHI·∫æU =================
# M·∫∑c ƒë·ªãnh theo d√µi HPG. B·∫°n c√≥ th·ªÉ th√™m m√£ kh√°c:
# STOCKS = [{"symbol": "HPG", "company": "H√≤a Ph√°t"}, {"symbol": "FPT", "company": "FPT"}]
STOCKS = [
    {
        "symbol": "HPG",
        "company": "H√≤a Ph√°t",
        # Alias ƒë·ªÉ b·∫Øt c√°c tin kh√¥ng ghi HPG nh∆∞ng li√™n quan h·ªá sinh th√°i (vd: HPA)
        "aliases": [
            "HPA",
            "N√¥ng nghi·ªáp H√≤a Ph√°t",
            "Hoa Phat Agriculture",
            "Tr·∫ßn ƒê√¨nh Long",
            "Dung Qu·∫•t",
            "Khu li√™n h·ª£p Dung Qu·∫•t",
            "H√≤a Ph√°t Dung Qu·∫•t",
        ],
    },
]

# ================= CH·∫æ ƒê·ªò CH·∫†Y / CH·ªêNG SPAM =================
# Qu√©t tin trong N ng√†y g·∫ßn nh·∫•t (m·∫∑c ƒë·ªãnh 30)
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "30"))
# Qu√©t s√¢u N b√†i g·∫ßn nh·∫•t tr√™n m·ªói RSS (m·∫∑c ƒë·ªãnh 200)
SCAN_PER_FEED = int(os.getenv("SCAN_PER_FEED", "200"))
# Gi·ªõi h·∫°n s·ªë tin g·ª≠i trong m·ªói l·∫ßn ch·∫°y (m·∫∑c ƒë·ªãnh 5) ƒë·ªÉ ch·∫°y th·ª≠ 1 th√°ng kh√¥ng spam
MAX_SEND_PER_RUN = int(os.getenv("MAX_SEND_PER_RUN", "5"))
# N·∫øu ƒë·∫∑t DRY_RUN=1 th√¨ ch·ªâ in ra, kh√¥ng g·ª≠i Telegram
DRY_RUN = os.getenv("DRY_RUN", "0").strip() == "1"

# ================= THI·∫æT L·∫¨P GEMINI / GEMMA 3 =================
# M·∫∑c ƒë·ªãnh d√πng Gemma 3 b·∫£n m·∫°nh nh·∫•t hi·ªán t·∫°i (27B, instruction-tuned).
# C√≥ th·ªÉ override b·∫±ng bi·∫øn m√¥i tr∆∞·ªùng GENAI_MODEL n·∫øu mu·ªën:
#   export GENAI_MODEL="gemma-3-12b-it"  (v√≠ d·ª•)
GENAI_MODEL_NAME = os.getenv("GENAI_MODEL", "gemma-3-27b-it").strip()

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GENAI_MODEL_NAME)
else:
    model = None

def send_telegram(message):
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        raise RuntimeError("Thi·∫øu TELEGRAM_TOKEN ho·∫∑c TELEGRAM_CHAT_ID (set bi·∫øn m√¥i tr∆∞·ªùng).")
    if DRY_RUN:
        print("[DRY_RUN] Would send Telegram message:")
        print(message[:800])
        return
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "Markdown"
    }
    try:
        requests.post(url, json=payload)
    except Exception as e:
        print(f"L·ªói g·ª≠i Telegram: {e}")

def normalize_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip()).lower()

def strip_html(s: str) -> str:
    # RSS snippet th∆∞·ªùng l√† HTML
    return re.sub(r"<[^>]+>", " ", s or "")

def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFKD", s or "")
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def contains_code(text: str, code: str) -> bool:
    if not code:
        return False
    return re.search(rf"\b{re.escape(code.upper())}\b", text, flags=re.IGNORECASE) is not None

# B·ªô l·ªçc b·ªè tin ph√°i sinh/ch·ª©ng quy·ªÅn
DERIV_KW = [
    "ch·ª©ng quy·ªÅn",
    "cw.",
    "cw/",
    "covered warrant",
    "ph√°i sinh",
    "cw hpg",
    "chpg",  # m√£ cw HPG th∆∞·ªùng c√≥ CHPGxxxx
]

def is_derivative_news(title: str, summary: str) -> bool:
    text = normalize_text(f"{title} {strip_html(summary)}")
    for kw in DERIV_KW:
        if kw in text:
            return True
    return False


def fetch_full_article(link: str, max_chars: int = 8000) -> str:
    """Crawl n·ªôi dung ch√≠nh c·ªßa b√†i b√°o ƒë·ªÉ AI ƒë·ªçc ƒë·∫ßy ƒë·ªß h∆°n."""
    if not link:
        return ""
    if not HAS_BS4:
        return ""
    try:
        resp = requests.get(link, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f"L·ªói t·∫£i b√†i b√°o: {e}")
        return ""

    try:
        soup = BeautifulSoup(resp.text, "html.parser")

        # Th·ª≠ m·ªôt s·ªë selector ph·ªï bi·∫øn cho b√°o VN
        candidates = [
            ".detail__content",
            ".ArticleContent",
            ".article__content",
            ".article-body",
            ".content-detail",
            ".article-content",
            "article",
        ]
        text = ""
        for sel in candidates:
            node = soup.select_one(sel)
            if node:
                text = node.get_text(" ", strip=True)
                break
        if not text:
            # fallback: d√πng to√†n b·ªô body
            body = soup.body or soup
            text = body.get_text(" ", strip=True)

        text = re.sub(r"\s+", " ", text).strip()
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        return text
    except Exception as e:
        print(f"L·ªói parse b√†i b√°o: {e}")
        return ""
def fingerprint(title: str, summary: str) -> str:
    # Dedup theo title + snippet (chu·∫©n ho√°) ƒë·ªÉ nhi·ªÅu b√°o ƒëƒÉng gi·ªëng nhau ch·ªâ g·ª≠i 1 l·∫ßn
    t = normalize_text(title)
    s = normalize_text(summary)
    # b·ªè link tracking / k√Ω t·ª± l·∫°
    base = re.sub(r"[^a-z0-9\u00c0-\u1ef9 ]+", " ", f"{t} {s}")
    base = re.sub(r"\s+", " ", base).strip()
    return hashlib.md5(base.encode("utf-8")).hexdigest()

# L·ªçc tin theo m√£/t√™n c√¥ng ty/alias
def is_stock_news(title: str, stock_cfg: dict, summary: str = "") -> bool:
    raw = f"{title} {strip_html(summary)}"
    text = normalize_text(raw)
    text_na = normalize_text(strip_accents(raw))

    symbol = (stock_cfg.get("symbol", "") or "").strip().upper()
    company = (stock_cfg.get("company", "") or "").strip()
    aliases = stock_cfg.get("aliases", []) or []

    # match m√£ c·ªï phi·∫øu theo word-boundary
    if symbol and (contains_code(raw, symbol) or contains_code(strip_accents(raw), symbol)):
        return True

    # match t√™n c√¥ng ty
    if company:
        c = normalize_text(company)
        c_na = normalize_text(strip_accents(company))
        if c in text or c_na in text_na:
            return True

    # match aliases (vd HPA, Tr·∫ßn ƒê√¨nh Long,...)
    for a in aliases:
        a = (a or "").strip()
        if not a:
            continue
        if len(a) <= 5 and a.isalnum():
            if contains_code(raw, a) or contains_code(strip_accents(raw), a):
                return True
            continue
        a_norm = normalize_text(a)
        a_na = normalize_text(strip_accents(a))
        if a_norm in text or a_na in text_na:
            return True

    return False

def is_within_days(entry, days: int) -> bool:
    try:
        # L·∫•y th·ªùi gian t·ª´ b√†i vi·∫øt (struct_time)
        if hasattr(entry, 'published_parsed'):
            published_time = entry.published_parsed
            pub_dt = datetime(
                published_time.tm_year,
                published_time.tm_mon,
                published_time.tm_mday,
                published_time.tm_hour,
                published_time.tm_min,
                published_time.tm_sec,
            )
            cutoff = datetime.now() - timedelta(days=days)
            return pub_dt >= cutoff
        # N·∫øu kh√¥ng c√≥ published_parsed, v·∫´n cho qua ƒë·ªÉ kh√¥ng b·ªè l·ª° (nh∆∞ng s·∫Ω dedup)
        return True
    except:
        return True

def build_google_queries(stock_cfg: dict) -> list:
    """X√¢y query Google News cho t·ª´ng m√£ + alias + domain."""
    terms = []
    symbol = (stock_cfg.get("symbol", "") or "").strip().upper()
    company = (stock_cfg.get("company", "") or "").strip()
    aliases = stock_cfg.get("aliases", []) or []

    if symbol:
        terms.append(symbol)
    if company:
        terms.append(company)
    for a in aliases:
        a = (a or "").strip()
        if a:
            terms.append(a)

    if not terms:
        return []

    base = "(" + " OR ".join(f'"{t}"' for t in terms) + ")"
    queries = []
    for domain in GOOGLE_SOURCES:
        queries.append(f"{base} site:{domain}")
    # th√™m 1 query r·ªông kh√¥ng domain ƒë·ªÉ ph√≤ng khi ngu·ªìn kh√°c c√≥ tin hay
    queries.append(base)
    return queries

def fetch_google_news(query: str, max_items: int) -> list:
    """L·∫•y RSS t·ª´ Google News search."""
    q = quote_plus(query)
    url = f"https://news.google.com/rss/search?q={q}&hl=vi&gl=VN&ceid=VN:vi"
    try:
        resp = requests.get(url, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"L·ªói g·ªçi Google News: {e}")
        return []

    import feedparser
    feed = feedparser.parse(resp.text)
    return feed.entries[:max_items]

def process_news():
    print(f"--- B·∫ÆT ƒê·∫¶U QU√âT TIN (LOOKBACK {LOOKBACK_DAYS} NG√ÄY): {datetime.now().strftime('%d/%m/%Y')} ---")
    count = 0
    if not model:
        print("Thi·∫øu GEMINI_API_KEY. H√£y set GEMINI_API_KEY ƒë·ªÉ b·∫≠t ph√¢n t√≠ch AI.")
        return

    # Dedup to√†n c·ª•c trong 1 l·∫ßn ch·∫°y (title+snippet)
    seen_fp = set()

    for stock_cfg in STOCKS:
        symbol = stock_cfg.get("symbol", "N/A")
        print(f"=== QU√âT TIN CHO {symbol} ===")
        queries = build_google_queries(stock_cfg)
        print(f"Queries: {queries}")

        for q in queries:
            print(f"Dang tim: {q} ...")
            entries = fetch_google_news(q, max_items=SCAN_PER_FEED)

            for entry in entries:
                if not is_within_days(entry, LOOKBACK_DAYS):
                    continue

                title = entry.title
                link = entry.link
                summary = getattr(entry, "summary", "")

                fp = fingerprint(title, summary)
                if fp in seen_fp:
                    continue

                # B·ªè tin ph√°i sinh / ch·ª©ng quy·ªÅn
                if is_derivative_news(title, summary):
                    continue

                if not is_stock_news(title, stock_cfg, summary=summary):
                    continue

                company = stock_cfg.get("company", "")
                print(f"-> TIM THAY ({symbol}): {title}")

                # L·∫•y full content c·ªßa b√†i ƒë·ªÉ ph√¢n t√≠ch ch√≠nh x√°c h∆°n
                article_text = fetch_full_article(link, max_chars=8000)

                prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch doanh nghi·ªáp/ch·ª©ng kho√°n Vi·ªát Nam.
H√£y ph√¢n t√≠ch tin sau (n·∫øu l√† ti·∫øng Anh h√£y d·ªãch v√† t√≥m t·∫Øt b·∫±ng ti·∫øng Vi·ªát) v√† ƒë√°nh gi√° ·∫£nh h∆∞·ªüng ƒë·∫øn c·ªï phi·∫øu {symbol} {f'({company})' if company else ''}.

Ti√™u ƒë·ªÅ: {title}
T√≥m t·∫Øt/RSS snippet (n·∫øu c√≥): {strip_html(summary)}
N·ªôi dung b√†i b√°o (ƒë√£ tr√≠ch): {article_text or '[Kh√¥ng tr√≠ch ƒë∆∞·ª£c n·ªôi dung, h√£y d·ª±a tr√™n ti√™u ƒë·ªÅ v√† snippet]'}
Link: {link}

Y√™u c·∫ßu output (Ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng):
1) üßæ **T√≥m t·∫Øt 1-2 c√¢u**
2) üéØ **·∫¢nh h∆∞·ªüng t·ªõi doanh nghi·ªáp/c·ªï phi·∫øu**: T√≠ch c·ª±c / Trung t√≠nh / Ti√™u c·ª±c
3) üìà **M·ª©c ƒë·ªô ·∫£nh h∆∞·ªüng**: Th·∫•p / Trung b√¨nh / Cao (k√®m l√Ω do)
4) üîé **ƒêi·ªÅu c·∫ßn theo d√µi ti·∫øp**: 2-3 bullet
5) ‚ö†Ô∏è **R·ªßi ro/gi·∫£ ƒë·ªãnh**: 1-2 bullet (n·∫øu c√≥)
"""

                try:
                    response = model.generate_content(prompt)
                    analysis = response.text.strip()
                    seen_fp.add(fp)

                    msg = (
                        f"üîî TIN C·ªî PHI·∫æU {symbol}\n\n"
                        f"{title}\n\n"
                        f"Snippet: {strip_html(summary).strip()[:280]}\n\n"
                        f"{analysis}\n\n"
                        f"Xem g·ªëc: {link}"
                    )
                    send_telegram(msg)
                    count += 1
                    time.sleep(3)

                    if count >= MAX_SEND_PER_RUN:
                        print(f"ƒê√£ ƒë·∫°t gi·ªõi h·∫°n g·ª≠i {MAX_SEND_PER_RUN} tin trong 1 l·∫ßn ch·∫°y.")
                        break
                except Exception as e:
                    print(f"L·ªói Gemini: {e}")

            if count >= MAX_SEND_PER_RUN:
                break

    if count == 0:
        print(f"Kh√¥ng c√≥ tin c·ªï phi·∫øu n√†o trong {LOOKBACK_DAYS} ng√†y g·∫ßn nh·∫•t (theo b·ªô l·ªçc hi·ªán t·∫°i).")
    else:
        print(f"ƒê√£ g·ª≠i {count} tin.")

if __name__ == "__main__":
    process_news()